<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Chaitanya Malaviya | publications and reports</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Chaitanya</strong> Malaviya
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/photos/">photos</a>
          
        
          
            <a class="page-link" href="/projects/">projects</a>
          
        
          
            <a class="page-link" href="/publications/">publications and reports</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications and reports</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content publications and reports clearfix">
    <h2>publications</h2>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="P18-2059">
  
    <span class="title"><a href="http://aclweb.org/anthology/P18-2059">Sparse and Constrained Attention for Neural Machine Translation</a></span>
    <span class="author">
      
        
          
            <em>Malaviya, Chaitanya</em>,
          
        
      
        
          
            
              Ferreira, Pedro,
            
          
        
      
        
          and
          
            
              <a href="https://andre-martins.github.io" target="_blank">Martins, Andr√© F. T.</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a class="slides" href="https://drive.google.com/open?id=1MrZQ1c69nygzt5duaGMKh8oZAH-HjErY">Slides</a>]
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>"In NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs."</p>
  </span>
  
</div>
</li>
<li>

<div id="malaviya-gormley-neubig:2018:Long">
  
    <span class="title"><a href="http://www.aclweb.org/anthology/P18-1247">Neural Factor Graph Models for Cross-lingual Morphological Tagging</a></span>
    <span class="author">
      
        
          
            <em>Malaviya, Chaitanya</em>,
          
        
      
        
          
            
              <a href="https://www.cs.cmu.edu/~mgormley" target="_blank">Gormley, Matthew R.</a>,
            
          
        
      
        
          and
          
            
              <a href="https://phontron.com" target="_blank">Neubig, Graham</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a class="slides" href="https://drive.google.com/open?id=1y_CBpyYAr0eV3oOi_5Eu0oNB2StK47Gj">Slides</a>]
  
  
    [<a class="video" href="https://www.youtube.com/watch?v=5jrDzqOFjBs">Video</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Morphological analysis involves predicting the syntactic traits of a word (e.g. POS: Noun, Case: Acc, Gender: Fem). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2017</h3>
<ol class="bibliography"><li>

<div id="malaviya-neubig-littell:2017:EMNLP2017">
  
    <span class="title"><a href="https://www.aclweb.org/anthology/D17-1268">Learning Language Representations for Typology Prediction</a></span>
    <span class="author">
      
        
          
            <em>Malaviya, Chaitanya</em>,
          
        
      
        
          
            
              <a href="https://phontron.com" target="_blank">Neubig, Graham</a>,
            
          
        
      
        
          and
          
            
              <a href="http://littell.nfshost.com" target="_blank">Littell, Patrick</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
    [<a class="poster" href="https://drive.google.com/open?id=1MJPG8zh6Ub0plWS7J3O-h1aPlayEOuxV">Poster</a>]
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>One central mystery of neural NLP is what neural models "know" about their
subject matter. When a neural machine translation system learns to translate
from one language to another, does it learn the syntax or semantics of the
languages? Can this knowledge be extracted from the system to fill holes in
human scientific knowledge? Existing typological databases contain relatively
full feature specifications for only a few hundred languages. Exploiting the
existence of parallel texts in more than a thousand languages, we build a
massive many-to-one NMT system from 1017 languages into English, and use this
to predict information missing from typological databases. Experiments show
that the proposed method is able to infer not only syntactic, but also
phonological and phonetic inventory features, and improves over a baseline that
has access to information about the languages geographic and phylogenetic
neighbors.</p>
  </span>
  
</div>
</li>
<li>

<div id="neubig17dynet">
  
    <span class="title"><a href="https://arxiv.org/abs/1701.03980">DyNet: The Dynamic Neural Network Toolkit</a></span>
    <span class="author">
      
        
          
            
              <a href="https://phontron.com" target="_blank">Neubig, Graham</a>,
            
          
        
      
        
          
            
              Dyer, Chris,
            
          
        
      
        
          
            
              Goldberg, Yoav,
            
          
        
      
        
          
            
              Matthews, Austin,
            
          
        
      
        
          
            
              Ammar, Waleed,
            
          
        
      
        
          
            
              Anastasopoulos, Antonios,
            
          
        
      
        
          
            
              Ballesteros, Miguel,
            
          
        
      
        
          
            
              Chiang, David,
            
          
        
      
        
          
            
              Clothiaux, Daniel,
            
          
        
      
        
          
            
              Cohn, Trevor,
            
          
        
      
        
          
            
              Duh, Kevin,
            
          
        
      
        
          
            
              Faruqui, Manaal,
            
          
        
      
        
          
            
              Gan, Cynthia,
            
          
        
      
        
          
            
              Garrette, Dan,
            
          
        
      
        
          
            
              Ji, Yangfeng,
            
          
        
      
        
          
            
              Kong, Lingpeng,
            
          
        
      
        
          
            
              Kuncoro, Adhiguna,
            
          
        
      
        
          
            
              Kumar, Gaurav,
            
          
        
      
        
          
            <em>Malaviya, Chaitanya</em>,
          
        
      
        
          
            
              Michel, Paul,
            
          
        
      
        
          
            
              Oda, Yusuke,
            
          
        
      
        
          
            
              Richardson, Matthew,
            
          
        
      
        
          
            
              Saphra, Naomi,
            
          
        
      
        
          
            
              Swayamdipta, Swabha,
            
          
        
      
        
          and
          
            
              Yin, Pengcheng
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>ArXiv</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet‚Äôs dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet‚Äôs speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license.</p>
  </span>
  
</div>
</li>
<li>

<div id="prabhumoye2017building">
  
    <span class="title"><a href="https://s3.amazonaws.com/alexaprize/2017/technical-article/magnus.pdf">Building CMU Magnus from User Feedback</a></span>
    <span class="author">
      
        
          
            
              Prabhumoye, Shrimai,
            
          
        
      
        
          
            
              Botros, Fadi,
            
          
        
      
        
          
            
              Chandu, Khyathi,
            
          
        
      
        
          
            
              Choudhary, Samridhi,
            
          
        
      
        
          
            
              Keni, Esha,
            
          
        
      
        
          
            <em>Malaviya, Chaitanya</em>,
          
        
      
        
          
            
              Manzini, Thomas,
            
          
        
      
        
          
            
              Pasumarthi, Rama,
            
          
        
      
        
          
            
              Poddar, Shivani,
            
          
        
      
        
          
            
              Ravichander, Abhilasha,
            
          
        
      
        
          and
          
            
              others, 
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Alexa Prize Proceedings</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent years have seen a surge in consumer usage of spoken dialog systems, due
to the popularity of voice assistants. While these systems are capable of answering
factual questions or executing basic tasks, they do not yet have the capability
to hold multi-turn conversations. The Alexa Prize challenge provides us a great
opportunity to explore various approaches and dialog strategies for building a
multi-turn conversational agent. In this report we identify key challenges to build
a social conversational dialog system, and present CMU Magnus, an intelligent
interactive spoken dialog system that can hold conversations over a range of topics.
The system learns and updates itself over time, and can handle argumentative or
subjective conversations.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2016</h3>
<ol class="bibliography"><li>

<div id="malaviya2016recsys">
  
    <span id="malaviya2016recsys">Malaviya, C. (2016). Recommender System for Events with Hybrid Filtering and Ensemble Machine Learning. <i>Bachelors‚Äô Thesis, Nanyang Technological University</i>. thesis.</span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Chaitanya Malaviya.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
